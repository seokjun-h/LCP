{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "\n",
    "# 시드 고정\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_100102/3224682186.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data[independent_vars] = scaler.fit_transform(train_data[independent_vars])\n",
      "/tmp/ipykernel_100102/3224682186.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data[independent_vars] = scaler.transform(test_data[independent_vars])\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/home/aibig25/hong_sj/trb/num.csv')\n",
    "data = data.fillna(0)\n",
    "\n",
    "unique_ids = data['sequence_ID'].unique()\n",
    "train_ids, test_ids = train_test_split(unique_ids, test_size=41, random_state=42)\n",
    "train_data = data[data['sequence_ID'].isin(train_ids)]\n",
    "test_data = data[data['sequence_ID'].isin(test_ids)]\n",
    "\n",
    "independent_vars = data.columns.difference(['center_x', 'center_y','center_x_ma','center_y_ma', 'ID', 'LC'])\n",
    "dependent_vars = ['center_y_ma']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "train_data[independent_vars] = scaler.fit_transform(train_data[independent_vars])\n",
    "test_data[independent_vars] = scaler.transform(test_data[independent_vars])\n",
    "\n",
    "X_train = train_data[independent_vars]\n",
    "y_train = train_data[dependent_vars]\n",
    "\n",
    "X_test = test_data[independent_vars]\n",
    "y_test = test_data[dependent_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 및 예측 시퀀스 길이 정의\n",
    "input_sequence_length = 60\n",
    "output_sequence_length = 30\n",
    "\n",
    "def create_sequences(data, input_sequence_length, output_sequence_length):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(data) - input_sequence_length - output_sequence_length + 1):\n",
    "        X.append(data.iloc[i:(i + input_sequence_length)][independent_vars].values)\n",
    "        y.append(data.iloc[(i + input_sequence_length):(i + input_sequence_length + output_sequence_length)][dependent_vars].values)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = create_sequences(train_data, input_sequence_length, output_sequence_length)\n",
    "X_test, y_test = create_sequences(test_data, input_sequence_length, output_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 텐서로 변환\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
    "\n",
    "# 데이터 로더 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_encoder_layers, num_decoder_layers, output_dim, lstm_hidden_dim, lstm_layers=1):\n",
    "        super(TrajectoryTransformer, self).__init__()\n",
    "        self.model_dim = model_dim\n",
    "        \n",
    "        # Linear layer to transform input dimensions to model dimensions\n",
    "        self.encoder = nn.Linear(input_dim, model_dim)\n",
    "        self.pos_encoder = PositionalEncoding(model_dim)\n",
    "        self.tgt_linear = nn.Linear(1, model_dim)  # Linear layer for transforming tgt dimensions\n",
    "        \n",
    "        # Define transformer encoder and decoder\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(model_dim, num_heads, lstm_hidden_dim, lstm_layers)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(model_dim, num_heads, lstm_hidden_dim, lstm_layers)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "        \n",
    "        self.decoder = nn.Linear(model_dim, output_dim)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # Encoding source input\n",
    "        src = self.encoder(src)\n",
    "        src = src * math.sqrt(self.model_dim)\n",
    "        src = self.pos_encoder(src.permute(1, 0, 2))  # Add positional encoding\n",
    "\n",
    "        # Processing target input\n",
    "        tgt = tgt.squeeze(-1)\n",
    "        original_shape = tgt.shape\n",
    "        tgt = tgt.reshape(-1, 1)\n",
    "        tgt = self.tgt_linear(tgt)\n",
    "        tgt = tgt.view(original_shape[0], original_shape[1], -1)\n",
    "        tgt = tgt * math.sqrt(self.model_dim)\n",
    "        tgt = self.pos_encoder(tgt.permute(1, 0, 2))  # Add positional encoding\n",
    "\n",
    "        # Apply encoder and decoder layers\n",
    "        for layer in self.encoder_layers:\n",
    "            src = layer(src)\n",
    "        \n",
    "        for layer in self.decoder_layers:\n",
    "            tgt = layer(tgt, src)\n",
    "\n",
    "        # Decode output and return\n",
    "        output = self.decoder(tgt.permute(1, 0, 2))\n",
    "        return output\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, lstm_hidden_dim, lstm_layers):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(model_dim, num_heads, dropout=0.1)\n",
    "        \n",
    "        # LSTM instead of Feedforward layer\n",
    "        self.lstm = nn.LSTM(input_size=model_dim, hidden_size=lstm_hidden_dim, num_layers=lstm_layers, batch_first=True, dropout=0.1)\n",
    "        self.linear = nn.Linear(lstm_hidden_dim, model_dim)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(model_dim)\n",
    "        self.norm2 = nn.LayerNorm(model_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Self-attention\n",
    "        src2 = self.self_attn(src, src, src)[0]\n",
    "        src = src + self.dropout(src2)\n",
    "        src = self.norm1(src)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, _ = self.lstm(src.permute(1, 0, 2))  # LSTM requires (batch_size, seq_length, input_size)\n",
    "        lstm_out = lstm_out.permute(1, 0, 2)  # Transpose back to (seq_length, batch_size, input_size)\n",
    "        \n",
    "        # Linear transformation\n",
    "        src2 = self.linear(lstm_out)\n",
    "        src = src + self.dropout(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, lstm_hidden_dim, lstm_layers):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(model_dim, num_heads, dropout=0.1)\n",
    "        self.multihead_attn = nn.MultiheadAttention(model_dim, num_heads, dropout=0.1)\n",
    "        \n",
    "        # LSTM instead of Feedforward layer\n",
    "        self.lstm = nn.LSTM(input_size=model_dim, hidden_size=lstm_hidden_dim, num_layers=lstm_layers, batch_first=True, dropout=0.1)\n",
    "        self.linear = nn.Linear(lstm_hidden_dim, model_dim)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(model_dim)\n",
    "        self.norm2 = nn.LayerNorm(model_dim)\n",
    "        self.norm3 = nn.LayerNorm(model_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, tgt, memory):\n",
    "        # Self-attention for the target sequence\n",
    "        tgt2 = self.self_attn(tgt, tgt, tgt)[0]\n",
    "        tgt = tgt + self.dropout(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        \n",
    "        # Cross-attention with encoder memory\n",
    "        tgt2 = self.multihead_attn(tgt, memory, memory)[0]\n",
    "        tgt = tgt + self.dropout(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, _ = self.lstm(tgt.permute(1, 0, 2))  # LSTM requires (batch_size, seq_length, input_size)\n",
    "        lstm_out = lstm_out.permute(1, 0, 2)  # Transpose back to (seq_length, batch_size, input_size)\n",
    "        \n",
    "        # Linear transformation\n",
    "        tgt2 = self.linear(lstm_out)\n",
    "        tgt = tgt + self.dropout(tgt2)\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "# 위치 인코딩 추가\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, model_dim, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, model_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, model_dim, 2).float() * (-math.log(10000.0) / model_dim))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', self.encoding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aibig25/anaconda3/envs/solar/lib/python3.8/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "input_dim = len(independent_vars)\n",
    "output_dim = len(dependent_vars)\n",
    "model_dim = 160\n",
    "num_heads = 5\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "lstm_hidden_dim = 50\n",
    "lstm_layers = 1\n",
    "\n",
    "model = TrajectoryTransformer(input_dim, model_dim, num_heads, num_encoder_layers, num_decoder_layers, output_dim, lstm_hidden_dim, lstm_layers)\n",
    "\n",
    "# 옵티마이저와 손실 함수\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, model_class, *model_args, **model_kwargs):\n",
    "    model = model_class(*model_args, **model_kwargs)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()  # 평가 모드로 설정\n",
    "    return model\n",
    "\n",
    "def get_attention_weights(model, src, tgt):\n",
    "    attention_weights = []\n",
    "    \n",
    "    src = model.encoder(src)\n",
    "    src = src * math.sqrt(model.model_dim)\n",
    "    src = model.pos_encoder(src.permute(1, 0, 2))\n",
    "\n",
    "    tgt = tgt.squeeze(-1)\n",
    "    original_shape = tgt.shape\n",
    "    tgt = tgt.reshape(-1, 1)\n",
    "    tgt = model.tgt_linear(tgt)\n",
    "    tgt = tgt.view(original_shape[0], original_shape[1], -1)\n",
    "    tgt = tgt * math.sqrt(model.model_dim)\n",
    "    tgt = model.pos_encoder(tgt.permute(1, 0, 2))\n",
    "\n",
    "    for layer in model.encoder_layers:\n",
    "        src = layer(src)\n",
    "        attention_weights.append(layer.self_attn(src, src, src)[1].detach().cpu().numpy())\n",
    "    \n",
    "    for layer in model.decoder_layers:\n",
    "        tgt = layer(tgt, src)\n",
    "        attention_weights.append(layer.self_attn(tgt, tgt, tgt)[1].detach().cpu().numpy())\n",
    "\n",
    "    return attention_weights\n",
    "\n",
    "def plot_attention_heatmap(attention_weights, input_seq_num, input_seq_length, output_seq_length, title, save_path=None):\n",
    "    avg_attention_weights = np.mean(attention_weights, axis=0).squeeze()\n",
    "\n",
    "    if avg_attention_weights.ndim == 3:\n",
    "        avg_attention_weights = avg_attention_weights.mean(axis=0)\n",
    "\n",
    "    # input sequence가 x축에, output sequence가 y축에 오도록 설정\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(avg_attention_weights[:output_seq_length, :input_seq_length], cmap='viridis', \n",
    "                xticklabels=range(1, input_seq_length + 1), \n",
    "                yticklabels=range(1, output_seq_length + 1))\n",
    "    plt.title(f'{title} (Input Sequence {input_seq_num})')\n",
    "    plt.xlabel('Input Sequence Position')\n",
    "    plt.ylabel('Output Sequence Position')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def visualize_attention_for_sequence(model, dataset, sequence_id, input_sequence_length, output_sequence_length, save_dir=None):\n",
    "    sequence_data = dataset[dataset['sequence_ID'] == sequence_id]\n",
    "    \n",
    "    X, y = create_sequences(sequence_data, input_sequence_length, output_sequence_length)\n",
    "    \n",
    "    src_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    tgt_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for i in range(src_tensor.size(0)):\n",
    "        src = src_tensor[i:i+1]\n",
    "        tgt = tgt_tensor[i:i+1]\n",
    "\n",
    "        attention_weights = get_attention_weights(model, src, tgt)\n",
    "        \n",
    "        if save_dir:\n",
    "            save_path = os.path.join(save_dir, f'seq_{sequence_id}_input_{i+1}.png')\n",
    "            plot_attention_heatmap(attention_weights[0], i + 1, input_sequence_length, output_sequence_length, \n",
    "                                   f'Attention Heatmap for Sequence ID {sequence_id}', save_path=save_path)\n",
    "        else:\n",
    "            plot_attention_heatmap(attention_weights[0], i + 1, input_sequence_length, output_sequence_length, \n",
    "                                   f'Attention Heatmap for Sequence ID {sequence_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델 로드\n",
    "model_path = \"/home/aibig25/hong_sj/trb/transformer/new/code/2_1.pth\"  \n",
    "model = load_model(model_path, TrajectoryTransformer, \n",
    "                   input_dim, model_dim, num_heads, num_encoder_layers, num_decoder_layers, output_dim, lstm_hidden_dim, lstm_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_id = 78\n",
    "save_directory = f'/home/aibig25/hong_sj/trb/transformer/new/heatmap/average/2 to 1/{sequence_id}'  \n",
    "visualize_attention_for_sequence(model, data, sequence_id, input_sequence_length, output_sequence_length, save_dir=save_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
